# Multimodal AI Assistant for Visual Accessibility

## Description:
This application assists visually impaired users by describing their surroundings through voice. The user can use simple voice commands such as “What’s around me?” or “Describe this scene.” The app captures an image or video using the device camera, processes it with a vision-language model, and provides a spoken description of what’s detected — people, objects, actions, and scene context.

It aims to create independence by enabling users to understand their environment hands-free, powered by AI-driven image captioning and text-to-speech (TTS) technologies.

## Problem Statement
Visually impaired individuals often rely on external assistance to understand their environment — especially in unfamiliar settings. Existing solutions are either expensive (specialized hardware) or limited in real-time feedback and natural interaction.
### Problem:
How can we provide a low-cost, real-time, voice-interactive system that describes a user's surroundings using just a smartphone camera and AI-based scene understanding?
### Goal:
To create an accessible mobile/web app that listens to voice commands, captures scenes, generates meaningful descriptions using computer vision, and delivers responses audibly in real time.

https://drive.google.com/file/d/1uo3vd6XRiNoLkBBqAghX51bVPaTcoUuP/view?usp=sharing
