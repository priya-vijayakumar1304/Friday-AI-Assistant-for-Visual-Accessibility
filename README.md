# Multimodal AI Assistant for Visual Accessibility

## Description:
This application assists visually impaired users by describing their surroundings through voice. The user can use simple voice commands such as “What’s around me?” or “Describe this scene.” an image captured using the device camera, processes it with a vision-language model, and provides a spoken description of what’s detected — people, objects, actions, and scene context.

It aims to create independence by enabling users to understand their environment, powered by AI-driven image captioning and text-to-speech (TTS) technologies.
[This project demonstrates **loading, saving and deploying** the `llava-hf/llava-interleave-qwen-0.5b-hf` model locally for **low-latency inference** using **Gradio**.]

## Demo

[![Video Title](https://img.youtube.com/vi/4nwuTbgo8pY/0.jpg)](https://www.youtube.com/watch?v=4nwuTbgo8pY)
