{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import necessary libraries"
      ],
      "metadata": {
        "id": "G0QvLBpVP5Qu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEun46ngOjZ7"
      },
      "outputs": [],
      "source": [
        "from transformers import LlavaProcessor, LlavaForConditionalGeneration\n",
        "import torch\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Loading the saved (from local drive) and using \"cuda\""
      ],
      "metadata": {
        "id": "Q2YGQI-YQBUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eIuzqRpL6Xuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOCAL_PATH = \"/content/drive/MyDrive/Friday_AI_Assistant/llava_local\""
      ],
      "metadata": {
        "id": "mbQ831EwO3EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"🔹 Using device: {device}\")"
      ],
      "metadata": {
        "id": "oFerw3jRO3GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = LlavaProcessor.from_pretrained(LOCAL_PATH, use_fast=True)\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "        LOCAL_PATH,\n",
        "        torch_dtype=torch.float16).to(device).eval()\n",
        "\n",
        "print(\"Model and processor loaded successfully.\")"
      ],
      "metadata": {
        "id": "TbeznHQsO3JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Scene Description with Gradio App"
      ],
      "metadata": {
        "id": "HLNQ7P-BQK-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import io\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "import tempfile\n",
        "import librosa\n",
        "import numpy as np\n",
        "import speech_recognition as sr\n",
        "\n",
        "# --- Speaker Verification Configuration ---\n",
        "# File path to save the voice signature (MFCC mean)\n",
        "VOICE_SIGNATURE_PATH = \"voice_signature.npy\"\n",
        "# Threshold for Euclidean distance (lower is closer match)\n",
        "VERIFICATION_THRESHOLD = 60.0\n",
        "\n",
        "# --- 1. Voice Signature Functions ---\n",
        "def extract_mfccs(audio_path: str) -> np.ndarray | None:\n",
        "    \"\"\"Extracts mean MFCC features from an audio file.\"\"\"\n",
        "    try:\n",
        "        # Load audio file (mono, 16kHz)\n",
        "        y, sr = librosa.load(audio_path, sr=16000)\n",
        "        # Extract MFCCs (13 coefficients by default)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        # Return the mean of all MFCCs across time (the 'signature')\n",
        "        return np.mean(mfccs.T, axis=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting MFCCs: {e}\")\n",
        "        return None\n",
        "\n",
        "def register_voice(audio_path: str | None) -> str:\n",
        "    \"\"\"Registers the speaker by saving their voice signature.\"\"\"\n",
        "    if audio_path is None:\n",
        "        return \"Registration failed: Please record a sample of your voice [5-10sec].\"\n",
        "\n",
        "    signature = extract_mfccs(audio_path)\n",
        "\n",
        "    if signature is not None:\n",
        "        try:\n",
        "            np.save(VOICE_SIGNATURE_PATH, signature)\n",
        "            return f\"Voice registered successfully! Signature saved to '{VOICE_SIGNATURE_PATH}'. You can now use the Friday AI Assistant. Go to next tab.\"\n",
        "        except Exception as e:\n",
        "            return f\"Registration failed: Could not save signature. Details: {e}\"\n",
        "    else:\n",
        "        return \"Registration failed: Could not process audio features.\"\n",
        "\n",
        "def verify_voice(audio_path: str | None) -> tuple[bool, str]:\n",
        "    \"\"\"Verifies the speaker against the stored signature.\"\"\"\n",
        "    if not os.path.exists(VOICE_SIGNATURE_PATH):\n",
        "        return False, \"Verification required: No voice signature found. Please register your voice first.\"\n",
        "\n",
        "    if audio_path is None:\n",
        "        return False, \"Verification failed: No audio provided for verification.\"\n",
        "\n",
        "    stored_signature = np.load(VOICE_SIGNATURE_PATH)\n",
        "    current_signature = extract_mfccs(audio_path)\n",
        "\n",
        "    if current_signature is None:\n",
        "        return False, \"Verification failed: Could not process input audio features.\"\n",
        "\n",
        "    # Calculate Euclidean distance between the stored and current signatures\n",
        "    distance = np.linalg.norm(stored_signature - current_signature)\n",
        "\n",
        "    # Simple verification logic: Check if distance is below the threshold\n",
        "    if distance < VERIFICATION_THRESHOLD:\n",
        "        return True, f\"✅ Verification successful! Distance: {distance:.2f} (Threshold: {VERIFICATION_THRESHOLD})\"\n",
        "    else:\n",
        "        return False, f\"🛑 Verification failed: Speaker mismatch detected. Distance: {distance:.2f} (Threshold: {VERIFICATION_THRESHOLD})\"\n",
        "\n",
        "\n",
        "# --- 2. Main Command Function ---\n",
        "\n",
        "def handle_input(audio_path: str, image: Image.Image) -> tuple[str, str | None]:\n",
        "    \"\"\"\n",
        "    Processes audio command and image input, now using Google Speech Recognition.\n",
        "    Returns: A tuple (text_response, audio_file_path_or_none)\n",
        "    \"\"\"\n",
        "    transcribed_text = \"\"\n",
        "    audio_file_path = None\n",
        "\n",
        "    # 1. SECURITY STEP: Verify the Speaker\n",
        "    is_verified, verification_status_message = verify_voice(audio_path)\n",
        "\n",
        "    if not is_verified:\n",
        "        # If verification fails, stop and return the failure message\n",
        "        return verification_status_message, None\n",
        "\n",
        "    # 2. Handle Audio Transcription using speech_recognition (only if verified)\n",
        "    if audio_path is None:\n",
        "        transcribed_text = \"No audio command received.\"\n",
        "    else:\n",
        "        r = sr.Recognizer()\n",
        "        try:\n",
        "            # Use sr.AudioFile to read the audio path provided by Gradio\n",
        "            with sr.AudioFile(audio_path) as source:\n",
        "                audio = r.record(source)\n",
        "                # Use Google Web Speech API for transcription\n",
        "                transcribed_text = r.recognize_google(audio).strip().lower()\n",
        "        except sr.UnknownValueError:\n",
        "            transcribed_text = \"Speech Recognition could not understand audio.\"\n",
        "        except sr.RequestError as e:\n",
        "            transcribed_text = f\"Could not request results from Google Speech Recognition service; {e}\"\n",
        "        except Exception as e:\n",
        "            transcribed_text = f\"General error during speech transcription: {e}\"\n",
        "\n",
        "\n",
        "    # 3. Handle Multimodal Command Logic\n",
        "    output_message = f\"{verification_status_message}\\n\\nCommand received: '{transcribed_text}'\"\n",
        "    image_status = \"Image successfully captured.\" if image is not None else \"Warning: No image captured.\"\n",
        "\n",
        "    #scene description using the model\n",
        "    try:\n",
        "        user_prompt = \"answer very precisely that matches the scene, answer in less than 70 words\" + transcribed_text\n",
        "        toks = \"<image>\" * 1\n",
        "        prompt = \"<|im_start|>user\"+ toks + f\"\\n{user_prompt}<|im_end|><|im_start|>assistant\"\n",
        "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device, model.dtype)\n",
        "        output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "        scene_description = processor.decode(output[0][2:], skip_special_tokens=True)[len(user_prompt)+10:]\n",
        "        # scene_description = \"A person sitting near a laptop on a table, possibly working.\"\n",
        "    except Exception as e:\n",
        "        scene_description = e\n",
        "\n",
        "    output_message += f\"\\n{image_status}\\n🧠 Scene Analysis: {scene_description}\"\n",
        "\n",
        "    # text-to-speech\n",
        "    narration_text = f\"The analysis of the captured scene is: {scene_description}\"\n",
        "    try:\n",
        "        tts = gTTS(text=narration_text, lang='en')\n",
        "        temp_file = tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False)\n",
        "        audio_file_path = temp_file.name\n",
        "        temp_file.close()\n",
        "        tts.save(audio_file_path)\n",
        "    except Exception as tts_e:\n",
        "        output_message += f\"\\n\\n🚨 TTS Error: Could not generate audio. Details: {tts_e}\"\n",
        "        audio_file_path = None\n",
        "\n",
        "\n",
        "    return output_message, audio_file_path\n",
        "\n",
        "# --- 3. Gradio Blocks Interface ---\n",
        "\n",
        "# --- Custom CSS for Styling ---\n",
        "CUSTOM_CSS = \"\"\"\n",
        "/* 1. Global Styles and Font */\n",
        ".gradio-container {\n",
        "    font-family: 'Inter', system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n",
        "    background-color: #f7f9fb; /* Lighter background */\n",
        "    padding: 20px;\n",
        "}\n",
        ".gradio-tabs {\n",
        "    border-radius: 20px; /* Larger radius */\n",
        "    box-shadow: 0 15px 40px rgba(0, 0, 0, 0.15); /* Stronger, softer shadow */\n",
        "    background-color: white;\n",
        "    padding: 30px; /* More internal padding */\n",
        "    border: none;\n",
        "}\n",
        "h1 {\n",
        "    color: #0d47a1; /* Deep blue title */\n",
        "    text-align: center;\n",
        "    font-weight: 700;\n",
        "    margin-bottom: 20px;\n",
        "}\n",
        "h2, h3 {\n",
        "    color: #374151; /* Subdued text color */\n",
        "    border-bottom: 1px solid #e5e7eb; /* Thinner, lighter separator */\n",
        "    padding-bottom: 8px;\n",
        "    margin-top: 15px;\n",
        "}\n",
        "\n",
        "/* 2. Tab Styling */\n",
        ".gradio-tabs button {\n",
        "    font-weight: 700;\n",
        "    border-radius: 10px 10px 0 0 !important;\n",
        "    color: #6b7280; /* Gray inactive tab text */\n",
        "    transition: all 0.2s ease-in-out;\n",
        "}\n",
        ".gradio-tabs button.selected {\n",
        "    color: #4c51bf !important; /* Indigo active tab text */\n",
        "    background-color: #f0f4f8 !important; /* Light background for active tab */\n",
        "    border-bottom: 3px solid #4c51bf !important; /* Highlight bottom border */\n",
        "}\n",
        "\n",
        "/* 3. Button Styling (Enhanced) */\n",
        ".gr-button {\n",
        "    background-color: #4c51bf; /* Primary indigo color */\n",
        "    color: white;\n",
        "    border: none;\n",
        "    border-radius: 12px; /* Slightly larger radius for button */\n",
        "    padding: 14px 25px; /* More padding */\n",
        "    font-weight: 700;\n",
        "    letter-spacing: 0.5px;\n",
        "    transition: all 0.3s ease;\n",
        "    text-transform: uppercase;\n",
        "}\n",
        ".gr-button:hover {\n",
        "    background-color: #3c40a5; /* Darker indigo on hover */\n",
        "    transform: translateY(-2px); /* Slight lift effect */\n",
        "    box-shadow: 0 8px 20px rgba(76, 81, 191, 0.5); /* Stronger shadow on hover */\n",
        "}\n",
        "\n",
        "/* 4. Input/Output Elements */\n",
        ".gr-textbox textarea, .gr-audio, .gr-image {\n",
        "    border-radius: 12px;\n",
        "    border: 1px solid #d1d5db; /* Lighter, cleaner border color */\n",
        "    background-color: #ffffff;\n",
        "    padding: 15px;\n",
        "    transition: border-color 0.3s;\n",
        "}\n",
        ".gr-textbox textarea:focus, .gr-audio:focus, .gr-image:focus {\n",
        "    border-color: #4c51bf; /* Highlight border on focus */\n",
        "    box-shadow: 0 0 0 2px rgba(76, 81, 191, 0.2);\n",
        "    outline: none;\n",
        "}\n",
        ".gr-audio-player audio {\n",
        "    border-radius: 12px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Friday, A Secure Multimodal Voice-Vision Assistant\", css=CUSTOM_CSS) as demo:\n",
        "    gr.Markdown(f\"# Friday, A Secure Multimodal AI Assistant\\n\")\n",
        "\n",
        "    with gr.Tab(\"💬 Friday, your AI Assistant\"):\n",
        "        gr.Markdown(\"## Scene description\\n **If you are using Friday the first time, please register your voice for authentication and security purpose**\")\n",
        "        gr.Markdown(\"\\nRecord a voice command (e.g., 'Friday, Describe the scene') and capture an image simultaneously. The assistant will transcribe the command, analyze the image, and narrate the description. **Only commands from the registered voice will be processed.**\")\n",
        "\n",
        "        with gr.Row(equal_height=True):\n",
        "            # Column 1: Inputs - Voice & Camera\n",
        "            with gr.Column(scale=1):\n",
        "                command_audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"🎙️ Voice Command\")\n",
        "                image_input = gr.Image(sources=[\"webcam\"], type=\"pil\", label=\"📸 Live Image Capture\")\n",
        "                run_button = gr.Button(\"▶️ Submit\")\n",
        "            # Column 2: Outputs - Text & Narration\n",
        "            with gr.Column(scale=1):\n",
        "                output_message_box = gr.Textbox(lines=10, label=\"Assistant Response\")\n",
        "                audio_output = gr.Audio(label=\"Scene Narration\", type=\"filepath\", autoplay=True)\n",
        "        run_button.click(\n",
        "            fn=handle_input,\n",
        "            inputs=[command_audio_input, image_input],\n",
        "            outputs=[output_message_box, audio_output]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"🎤 Voice Registration\"):\n",
        "        gr.Markdown(\"## Register Your Voice\\nRecord a short (5-10 second) sample of your voice to create your security signature.\")\n",
        "\n",
        "        with gr.Row(equal_height=True):\n",
        "            with gr.Column(scale=1):\n",
        "                register_audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Record Registration Sample\")\n",
        "                register_button = gr.Button(\"🔒 Save My Voice Signature\")\n",
        "            with gr.Column(scale=1):\n",
        "                registration_status = gr.Textbox(label=\"Registration Status\", lines=2, interactive=False)\n",
        "\n",
        "        register_button.click(\n",
        "            fn=register_voice,\n",
        "            inputs=[register_audio_input],\n",
        "            outputs=[registration_status]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(f\"Note: Your current voice signature is used to verify your identity when using the Friday AI Assistant.\")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "pQgrMKZEX84K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}